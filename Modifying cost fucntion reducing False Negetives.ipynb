{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "https://www.kaggle.com/uciml/adult-census-income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "census= pd.read_csv('adult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27504 entries, 1 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               27504 non-null int64\n",
      "workclass         27504 non-null object\n",
      "fnlwgt            27504 non-null int64\n",
      "education         27504 non-null object\n",
      "education.num     27504 non-null int64\n",
      "marital.status    27504 non-null object\n",
      "occupation        27504 non-null object\n",
      "relationship      27504 non-null object\n",
      "race              27504 non-null object\n",
      "sex               27504 non-null object\n",
      "capital.gain      27504 non-null int64\n",
      "capital.loss      27504 non-null int64\n",
      "hours.per.week    27504 non-null int64\n",
      "native.country    27504 non-null object\n",
      "income            27504 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.4+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjun/anaconda/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "census_1= census.mask(census.eq('?')).dropna()\n",
    "census_1= census_1[census['native.country']=='United-States']\n",
    "census_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing numeric columns using Min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "census_1[['age', 'fnlwgt','education.num','capital.gain','capital.loss','hours.per.week']] = scaler.fit_transform(census_1[['age', 'fnlwgt','education.num','capital.gain','capital.loss','hours.per.week']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<=50K    20509\n",
       ">50K      6995\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_1['income'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting data in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x,data_y = census_1.drop('income',axis=1), census_1['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One- Hot encoding on categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x= pd.get_dummies(data_x).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le= LabelEncoder()\n",
    "le.fit(data_y)\n",
    "data_y=le.transform(data_y)\n",
    "data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split (80-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 64)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.20, random_state=42)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping data into (n x m), n being the number of features and m being the number of training examples. Target label array is also reshaped into (1 x m) signifying 1 output and m examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= X_train.T\n",
    "y_train= y_train.reshape(1,22003)\n",
    "X_test = X_test.T\n",
    "y_test =y_test.reshape(1,5501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "Y = y_train\n",
    "\n",
    "n_x = X.shape[0] #number of input layers\n",
    "n_h = 2*n_x # number of hidden layers\n",
    "learning_rate = 0.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid and sigmoid derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "def der_sigmoid(z):\n",
    "    s= sigmoid(z) * (1 - sigmoid(z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Cross entropy Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_1(Y, Y_hat,a,b):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * ( np.sum(a* np.multiply(np.log(Y_hat),Y) ) + np.sum(b*np.multiply(np.log(1-Y_hat),(1-Y)) ) )\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network function (Numpy implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralnet_modified(activation,derivative,epochs,a,b):\n",
    "    np.random.seed(101)\n",
    "    W1 = np.random.randn(n_h, n_x)*np.sqrt(2/n_x)\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(1, n_h)*np.sqrt(2/n_h)\n",
    "    b2 = np.zeros((1, 1))\n",
    "    m = Y.shape[1]\n",
    "    act=activation\n",
    "    der=derivative\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        Z1 = np.matmul(W1, X) + b1\n",
    "        A1 = act(Z1)\n",
    "        Z2 = np.matmul(W2, A1) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        cost = compute_loss_1(Y, A2,a,b)\n",
    "\n",
    "        dZ2 = -a*np.multiply(Y,(1-A2))+ b*np.multiply(A2,(1-Y))\n",
    "        dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
    "        db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = np.matmul(W2.T, dZ2)\n",
    "        dZ1 = dA1* der(Z1)\n",
    "        dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
    "        db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        W2 = W2 - learning_rate * dW2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "\n",
    "       \n",
    "    \n",
    "    Z1 = np.matmul(W1, X_test) + b1\n",
    "    A1 = act(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    predictions = (A2>.5)[0,:]\n",
    "    labels = (y_test == 1)[0,:]\n",
    "    tn, fp, fn, tp = confusion_matrix(labels,predictions).ravel()\n",
    "    fpr= fp/(fp+tn)\n",
    "    fnr= fn/(fn+tp)\n",
    "    acc= accuracy_score(labels, predictions)\n",
    "    return(fpr,fnr,acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the value of threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "threshold = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR:  0.07149829184968277 FNR:  0.5110477548111191 Test Accuracy:  0.8163970187238684\n",
      "FPR:  0.09175207418252807 FNR:  0.45759087669280113 Test Accuracy:  0.8149427376840574\n",
      "FPR:  0.11078574914592484 FNR:  0.42052744119743407 Test Accuracy:  0.8102163243046718\n",
      "FPR:  0.12347486578818936 FNR:  0.383464005702067 Test Accuracy:  0.8102163243046718\n",
      "FPR:  0.148365056124939 FNR:  0.3435495367070563 Test Accuracy:  0.8018542083257589\n",
      "FPR:  0.16861883845778428 FNR:  0.3050605844618674 Test Accuracy:  0.7965824395564443\n",
      "FPR:  0.18570034163006344 FNR:  0.2758374910905203 Test Accuracy:  0.7913106707871296\n",
      "FPR:  0.20717423133235724 FNR:  0.24019957234497505 Test Accuracy:  0.7844028358480276\n",
      "FPR:  0.232796486090776 FNR:  0.20028510334996436 Test Accuracy:  0.7754953644791857\n",
      "FPR:  0.258174719375305 FNR:  0.17320028510334998 Test Accuracy:  0.7634975459007454\n",
      "FPR:  0.28184480234260617 FNR:  0.1525302922309337 Test Accuracy:  0.7511361570623523\n",
      "FPR:  0.3008784773060029 FNR:  0.12829650748396293 Test Accuracy:  0.7431376113433921\n",
      "FPR:  0.31234748657881894 FNR:  0.11261582323592302 Test Accuracy:  0.7385929830939829\n",
      "FPR:  0.3247925817471938 FNR:  0.10548823948681398 Test Accuracy:  0.7311397927649518\n",
      "FPR:  0.3365056124938995 FNR:  0.0962223806129722 Test Accuracy:  0.724777313215779\n",
      "FPR:  0.34602244997559783 FNR:  0.09052031361368496 Test Accuracy:  0.7191419741865115\n",
      "FPR:  0.35334309419228893 FNR:  0.08624376336421953 Test Accuracy:  0.7147791310670787\n",
      "FPR:  0.36066373840898 FNR:  0.08125445473984319 Test Accuracy:  0.7105980730776222\n",
      "FPR:  0.36627623230844314 FNR:  0.08054169636493229 Test Accuracy:  0.7065988002181421\n",
      "FPR:  0.3706686188384578 FNR:  0.07555238774055595 Test Accuracy:  0.7045991637884021\n",
      "FPR:  0.37457296242069305 FNR:  0.07341411261582323 Test Accuracy:  0.7022359570987093\n",
      "FPR:  0.37725719863347973 FNR:  0.07127583749109052 Test Accuracy:  0.7007816760588984\n",
      "FPR:  0.3821376281112738 FNR:  0.0691375623663578 Test Accuracy:  0.6976913288493001\n",
      "FPR:  0.38653001464128844 FNR:  0.0684248039914469 Test Accuracy:  0.6946009816397019\n",
      "FPR:  0.390190336749634 FNR:  0.06272273699215966 Test Accuracy:  0.6933284857298673\n",
      "FPR:  0.39604685212298685 FNR:  0.05773342836778332 Test Accuracy:  0.690238138520269\n",
      "FPR:  0.4011713030746706 FNR:  0.05559515324305061 Test Accuracy:  0.6869660061806944\n"
     ]
    }
   ],
   "source": [
    "a=1\n",
    "b=1\n",
    "epochs=400\n",
    "fpr,fnr,test_acc = neuralnet_modified(sigmoid,der_sigmoid,epochs,a,b)\n",
    "while (fpr< float(threshold)):\n",
    "    fpr,fnr,test_acc= neuralnet_modified(sigmoid,der_sigmoid,epochs,a,b)\n",
    "    print('FPR: ',fpr,'FNR: ',fnr, 'Test Accuracy: ',tagest_acc)\n",
    "    a=a+0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model achieved after 27 iterations with FPR = 0.40, has FNR= 0.055 and has an accuracy of 68.69%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking confusion matrix and model evaluators using the a value obtained (a=6.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralnet_modified_test(activation,derivative,epochs,a,b):\n",
    "    np.random.seed(101)\n",
    "    W1 = np.random.randn(n_h, n_x)*np.sqrt(2/n_x)\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(1, n_h)*np.sqrt(2/n_h)\n",
    "    b2 = np.zeros((1, 1))\n",
    "    m = Y.shape[1]\n",
    "    act=activation\n",
    "    der=derivative\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        Z1 = np.matmul(W1, X) + b1\n",
    "        A1 = act(Z1)\n",
    "        Z2 = np.matmul(W2, A1) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        cost = compute_loss_1(Y, A2,a,b)\n",
    "\n",
    "        dZ2 = -a*np.multiply(Y,(1-A2))+ b*np.multiply(A2,(1-Y))\n",
    "        dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
    "        db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = np.matmul(W2.T, dZ2)\n",
    "        dZ1 = dA1* der(Z1)\n",
    "        dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
    "        db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        W2 = W2 - learning_rate * dW2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "    print(\"Final cost:\", cost)\n",
    "    \n",
    "    Z1 = np.matmul(W1, X_train) + b1\n",
    "    A1 = act(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    predictions = (A2>.5)[0,:]\n",
    "    labels = (y_train == 1)[0,:]\n",
    "    print(confusion_matrix(labels,predictions))\n",
    "    print(classification_report(labels,predictions))\n",
    "    print('Train Accuracy =' , accuracy_score(labels, predictions))\n",
    "    \n",
    "    Z1 = np.matmul(W1, X_test) + b1\n",
    "    A1 = act(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    predictions = (A2>.5)[0,:]\n",
    "    labels = (y_test == 1)[0,:]\n",
    "    print(confusion_matrix(labels,predictions))\n",
    "    print(classification_report(labels,predictions))\n",
    "    print('Test Accuracy =' , accuracy_score(labels, predictions))\n",
    "    tn, fp, fn, tp = confusion_matrix(labels,predictions).ravel()\n",
    "    fpr= fp/(fp+tn)\n",
    "    fnr= fn/(fn+tp)\n",
    "    acc= accuracy_score(labels, predictions)\n",
    "    return(fpr,fnr,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  1.996789705916368\n",
      "Epoch 100 cost:  1.0048146717777364\n",
      "Epoch 200 cost:  0.947619566830966\n",
      "Epoch 300 cost:  0.9224960900714368\n",
      "Final cost: 0.906582293324698\n",
      "[[9872 6539]\n",
      " [ 346 5246]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.97      0.60      0.74     16411\n",
      "       True       0.45      0.94      0.60      5592\n",
      "\n",
      "avg / total       0.83      0.69      0.71     22003\n",
      "\n",
      "Train Accuracy = 0.68708812434668\n",
      "[[2454 1644]\n",
      " [  78 1325]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.97      0.60      0.74      4098\n",
      "       True       0.45      0.94      0.61      1403\n",
      "\n",
      "avg / total       0.84      0.69      0.71      5501\n",
      "\n",
      "Test Accuracy = 0.6869660061806944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4011713030746706, 0.05559515324305061, 0.6869660061806944)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuralnet_modified_test(sigmoid,der_sigmoid,400,6.2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion- \n",
    "By setting a threshold of 0.4 i.e 40% FPR we were able to reduce FNR to approximately 5.5% from 51% in the beginning. We can see that the accuracy drops each time we make an increment to a. In this case the accuracy dropped from 81.6 to 68.69 %. Significant computation effort was required to reach this threshold with 27 iterations and 400 epochs the total number of epochs were 10,800."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:- \n",
    "http://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
